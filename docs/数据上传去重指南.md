# 📊 数据上传去重指南

## ✨ 新功能概述

系统现已支持**智能去重**功能，确保不会重复导入相同的数据。

### 去重规则

系统使用以下组合作为唯一标识：
- **物流单号** (orderNumber)
- **出库日期** (date - 年-月-日)

只要这两个字段的组合相同，就会被判定为重复记录。

---

## 🚀 使用方法

### 方法一：先验证，后导入（推荐）⭐

#### 步骤 1：验证文件

在上传页面，点击 **"验证数据"** 按钮：

**API 端点**：`POST /api/upload/validate`

**返回信息**：
```json
{
  "success": true,
  "message": "文件验证完成",
  "data": {
    "file": {
      "totalRecords": 5000,           // 文件总记录数
      "uniqueRecords": 4800,          // 新的唯一记录
      "duplicateRecords": 200,        // 重复记录数
      "dateRange": {
        "min": "2024-10-27",
        "max": "2024-10-31"
      },
      "platforms": {                  // 平台分布
        "拼多多": 2000,
        "淘宝": 1500,
        "京东": 1500
      }
    },
    "currentDatabase": {
      "totalRecords": 161853,         // 当前数据库记录数
      "dateRange": {
        "min": "2024-10-01",
        "max": "2024-10-26"
      },
      "platforms": ["拼多多", "淘宝", "京东", "抖音", ...]
    },
    "duplicateSamples": [             // 重复记录示例（最多10条）
      {
        "orderNumber": "SF1234567890",
        "date": "2024-10-26",
        "platform": "拼多多",
        "cost": 8.50
      },
      ...
    ]
  }
}
```

#### 步骤 2：确认导入

如果验证结果满意，点击 **"确认导入"** 按钮：

**API 端点**：`POST /api/upload`

**返回信息**：
```json
{
  "success": true,
  "message": "成功导入 4800 条新记录，跳过 200 条重复记录",
  "recordsProcessed": 4800,
  "data": {
    "imported": 4800,               // 实际导入数
    "duplicates": 200,              // 跳过的重复数
    "total": 5000,                  // 文件总记录数
    "currentDatabase": {            // 导入后的数据库状态
      "totalRecords": 166653,
      "dateRange": {
        "min": "2024-10-01",
        "max": "2024-10-31"
      },
      "platforms": [...]
    }
  }
}
```

---

### 方法二：直接导入（自动去重）

直接上传文件，系统会自动去重并导入。

在上传页面，选择文件后点击 **"上传数据"** 按钮。

系统会：
1. ✅ 自动检测重复记录
2. ✅ 只导入新的记录
3. ✅ 在控制台显示详细统计

---

## 📊 后台日志示例

### 导入时的控制台输出

```bash
📦 开始处理上传文件...
  - 文件名: 报表运费10月27-31日.xlsx
  - 解析记录数: 5000

🔍 去重检查结果:
  - 重复记录: 200
  - 唯一记录: 4800

⚠️  重复记录示例 (前5条):
  1. SF1234567890 | 2024-10-26 | 拼多多 | ¥8.5
  2. YTO9876543210 | 2024-10-26 | 淘宝 | ¥6.8
  3. JD5544332211 | 2024-10-26 | 京东 | ¥9.2
  4. SF2233445566 | 2024-10-26 | 拼多多 | ¥7.5
  5. YTO7788990011 | 2024-10-26 | 淘宝 | ¥5.9

📊 数据导入统计:
  - 总记录数: 5000
  - 新增记录: 4800
  - 重复记录: 200
```

---

## 🔍 去重逻辑说明

### 为什么使用 "物流单号 + 日期" 作为唯一标识？

1. **物流单号** (orderNumber)
   - 每个订单的唯一标识
   - 物流公司分配，全局唯一

2. **出库日期** (date)
   - 年-月-日 级别的日期
   - 同一订单可能在不同天有不同状态

3. **组合判断**
   - `SF1234567890_2024-10-26`
   - `SF1234567890_2024-10-27`
   - 这两条记录会被视为**不同的记录**

### 不使用其他字段的原因

- ❌ 平台、运费、重量等可能会有更新
- ❌ 仅用物流单号可能丢失历史状态
- ✅ 物流单号 + 日期既保证唯一性，又保留历史

---

## ⚠️ 注意事项

### 1. 数据覆盖更新

如果需要**更新**已存在的记录（而不是去重），需要：
1. 先删除旧数据
2. 再导入新数据

或者，未来可以考虑添加 **"强制更新"** 功能。

### 2. 日期范围重叠

- 如果上传的文件日期范围与数据库重叠（如都包含10月26日）
- 系统会自动跳过重复的那一天的数据
- 只导入新的日期的数据

### 3. 大文件上传

- 当前限制：**50MB**（可在后端配置调整）
- 建议：每次上传不超过 **20万条记录**
- 如果文件过大，可以分批上传

### 4. Excel 文件格式

确保 Excel 文件包含以下必需字段：
- ✅ 物流单号
- ✅ 出库单时间
- ✅ 计算重量
- ✅ 运费
- ✅ 平台
- ✅ 订单金额

---

## 🧪 测试示例

### 测试场景 1：完全新数据

**上传文件**：`10月27-31日数据.xlsx`（10月27日到10月31日）  
**数据库现有**：10月1日到10月26日

**预期结果**：
- 重复记录：0
- 导入记录：全部

---

### 测试场景 2：部分重叠

**上传文件**：`10月25-31日数据.xlsx`（10月25日到10月31日）  
**数据库现有**：10月1日到10月26日

**预期结果**：
- 重复记录：10月25日和10月26日的数据
- 导入记录：10月27日到10月31日的数据

---

### 测试场景 3：完全重复

**上传文件**：`10月数据.xlsx`（10月1日到10月31日）  
**数据库现有**：10月1日到10月31日

**预期结果**：
- 重复记录：全部
- 导入记录：0
- 提示：所有记录已存在，无需导入

---

## 💡 最佳实践

1. **日常上传**
   - 每天上传当天的数据
   - 或者每周上传一次新数据
   - 避免大量重复

2. **数据补充**
   - 先使用 `/validate` 验证
   - 查看重复情况
   - 再决定是否导入

3. **定期备份**
   - 虽然系统不会删除数据
   - 但建议定期导出备份
   - 以防意外

4. **数据清理**
   - 如果需要重新导入全部数据
   - 可以先清空数据库
   - 再导入完整数据

---

## 🛠️ 技术实现

### 去重算法

```typescript
function getRecordKey(record: FreightRecord): string {
  const dateStr = new Date(record.date).toISOString().split('T')[0];
  return `${record.orderNumber}_${dateStr}`;
}

// 检查重复
const existingKeys = new Set<string>();
for (const record of existingRecords) {
  existingKeys.add(getRecordKey(record));
}

// 过滤新记录
const uniqueRecords = newRecords.filter(record => 
  !existingKeys.has(getRecordKey(record))
);
```

### 性能优化

- ✅ 使用 `Set` 数据结构，查询时间复杂度 O(1)
- ✅ 一次遍历完成去重，整体时间复杂度 O(n)
- ✅ 支持大数据量（20万+ 记录）

---

## 📞 问题反馈

如果遇到任何问题，请提供：
1. 上传的文件示例（前10行）
2. 返回的错误信息
3. 预期的导入结果
4. 实际的导入结果

这样可以更快地定位和解决问题！ 🚀

